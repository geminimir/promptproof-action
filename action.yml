name: 'PromptProof Eval'
description: 'Deterministic replay and policy checks for LLM outputs'
author: 'PromptProof'
branding:
  icon: 'check-circle'
  color: 'green'

inputs:
  config:
    description: 'Path to promptproof.yaml configuration file'
    required: false
    default: 'promptproof.yaml'
  format:
    description: 'Output format (console|html|junit|json|sarif)'
    required: false
    default: 'html'
  mode:
    description: 'Evaluation mode (gate|report-only|fail|warn). Defaults to policy mode.'
    required: false
    default: ''
  node-version:
    description: 'Node.js version to use'
    required: false
    default: '20'
  regress:
    description: 'Compare against baseline snapshot (true|false)'
    required: false
    default: ''
  baseline-ref:
    description: 'Git ref to load baseline snapshot from (e.g., origin/main). Implies regress.'
    required: false
    default: ''
  seed:
    description: 'Seed for non-deterministic checks'
    required: false
    default: ''
  runs:
    description: 'Number of runs for non-deterministic checks'
    required: false
    default: ''
  max-run-cost:
    description: 'Maximum total cost for this run in USD (overrides policy budgets.cost_usd_total_max)'
    required: false
    default: ''
  snapshot-on-success:
    description: 'Create a snapshot after a successful run (true|false)'
    required: false
    default: ''
  snapshot-promote-on-main:
    description: 'Promote snapshot to baseline when on main branch (true|false)'
    required: false
    default: ''
  snapshot-tag:
    description: 'Optional snapshot tag to use'
    required: false
    default: ''
  report-artifact:
    description: 'Name of the uploaded report artifact (without extension)'
    required: false
    default: 'promptproof-report'

outputs:
  violations:
    description: 'Number of violations found'
    value: ${{ steps.eval.outputs.violations }}
  passed:
    description: 'Number of fixtures that passed'
    value: ${{ steps.eval.outputs.passed }}
  failed:
    description: 'Number of fixtures that failed'
    value: ${{ steps.eval.outputs.failed }}
  report-path:
    description: 'Path to the generated report'
    value: ${{ steps.eval.outputs.report-path }}
  exit-code:
    description: 'Exit code from evaluation'
    value: ${{ steps.eval.outputs.exit-code }}
  total-cost:
    description: 'Total cost (USD) for the evaluated fixtures'
    value: ${{ steps.eval.outputs.total-cost }}
  failed-tests:
    description: 'Alias for failed (number of fixtures that failed)'
    value: ${{ steps.eval.outputs.failed-tests }}
  regressions:
    description: 'Number of new failures found versus baseline (when regression comparison is enabled)'
    value: ${{ steps.eval.outputs.regressions }}

runs:
  using: 'composite'
  steps:
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ inputs.node-version }}

    - name: Ensure PromptProof CLI available (via npx)
      shell: bash
      run: |
        echo "::group::Checking PromptProof availability"
        node -v
        npm -v
        # Prefer beta CLI to ensure latest checks/reporters are available
        if npx -y -p promptproof-cli@beta promptproof --version >/dev/null 2>&1; then
          echo "Using PromptProof CLI @beta"
          echo "CLI_PACKAGE=promptproof-cli@beta" >> "$GITHUB_ENV"
          npx -y -p promptproof-cli@beta promptproof --version || true
        else
          echo "Falling back to stable PromptProof CLI"
          echo "CLI_PACKAGE=promptproof-cli" >> "$GITHUB_ENV"
          npx -y -p promptproof-cli promptproof --version || true
        fi
        echo "::endgroup::"

    - name: Validate inputs
      shell: bash
      run: |
        set -e
        ERR=0
        # Validate format
        case "${{ inputs.format }}" in
          console|html|junit|json|sarif) ;;
          *) echo "::error title=Invalid input::format must be one of console|html|junit|json (got '${{ inputs.format }}')"; ERR=1;;
        esac
        # Validate mode synonyms
        case "${{ inputs.mode }}" in
          ""|gate|report-only|fail|warn) ;;
          *) echo "::error title=Invalid input::mode must be one of gate|report-only|fail|warn (got '${{ inputs.mode }}')"; ERR=1;;
        esac
        # Validate numeric inputs
        if [ -n "${{ inputs.runs }}" ] && ! echo "${{ inputs.runs }}" | grep -Eq '^[0-9]+$'; then
          echo "::error title=Invalid input::runs must be an integer (got '${{ inputs.runs }}')"; ERR=1
        fi
        if [ -n "${{ inputs.seed }}" ] && ! echo "${{ inputs.seed }}" | grep -Eq '^[0-9]+$'; then
          echo "::error title=Invalid input::seed must be an integer (got '${{ inputs.seed }}')"; ERR=1
        fi
        if [ -n "${{ inputs['max-run-cost'] }}" ] && ! echo "${{ inputs['max-run-cost'] }}" | grep -Eq '^[0-9]+(\.[0-9]+)?$'; then
          echo "::error title=Invalid input::max-run-cost must be a number (got '${{ inputs['max-run-cost'] }}')"; ERR=1
        fi
        # Validate config path
        if [ -n "${{ inputs.config }}" ] && [ ! -f "${{ inputs.config }}" ]; then
          echo "::warning title=Config not found::'${{ inputs.config }}' does not exist at checkout time; ensure the file is created before evaluation."
        fi
        if [ "$ERR" -ne 0 ]; then
          exit 2
        fi

    - name: Run PromptProof Evaluation
      id: eval
      shell: bash
      run: |
        echo "::group::Running PromptProof evaluation"
        
        # Determine effective config path (may be rewritten below)
        EFFECTIVE_CONFIG="${{ inputs.config }}"

        # Prepare artifact base name and output path
        ARTIFACT_NAME="${{ inputs['report-artifact'] }}"
        if [ -z "$ARTIFACT_NAME" ]; then
          ARTIFACT_NAME="promptproof-report"
        fi

        # Handle baseline-ref: fetch baseline snapshot files from the given ref
        if [ -n "${{ inputs['baseline-ref'] }}" ]; then
          echo "Fetching baseline snapshot from ref: ${{ inputs['baseline-ref'] }}"
          mkdir -p .promptproof/baselines
          # Find all last_green.json files at the ref
          SUITE_FILES=$(git ls-tree -r --name-only "${{ inputs['baseline-ref'] }}" .promptproof/baselines 2>/dev/null | grep 'last_green.json' || true)
          if [ -n "$SUITE_FILES" ]; then
            while read -r FILE; do
              [ -z "$FILE" ] && continue
              SUITE=$(echo "$FILE" | awk -F/ '{print $3}')
              mkdir -p ".promptproof/baselines/$SUITE"
              git show "${{ inputs['baseline-ref'] }}:$FILE" > ".promptproof/baselines/$SUITE/last_green.json" || true
              BASELINE_JSON=".promptproof/baselines/$SUITE/last_green.json"
              if [ -f "$BASELINE_JSON" ]; then
                MANIFEST_REL=$(node -e "try{const b=require('./$BASELINE_JSON');console.log(b.path||'')}catch(e){console.log('')}")
                TAG=$(node -e "try{const b=require('./$BASELINE_JSON');console.log(b.tag||'')}catch(e){console.log('')}")
                if [ -n "$MANIFEST_REL" ]; then
                  mkdir -p ".promptproof/baselines/$SUITE/$MANIFEST_REL"
                  git show "${{ inputs['baseline-ref'] }}:.promptproof/baselines/$SUITE/$MANIFEST_REL/manifest.json" > ".promptproof/baselines/$SUITE/$MANIFEST_REL/manifest.json" 2>/dev/null || true
                fi
                if [ -n "$TAG" ]; then
                  mkdir -p ".promptproof/snapshots/$SUITE/$TAG"
                  git show "${{ inputs['baseline-ref'] }}:.promptproof/snapshots/$SUITE/$TAG/manifest.json" > ".promptproof/snapshots/$SUITE/$TAG/manifest.json" 2>/dev/null || true
                fi
              fi
            done <<< "$SUITE_FILES"
          else
            echo "No baseline snapshots found at ref ${{ inputs['baseline-ref'] }}" || true
          fi
        fi

        # Possibly rewrite config to enforce mode and/or max-run-cost
        NEEDS_REWRITE="0"
        TMP_CONFIG="promptproof.effective.yaml"

        # Normalize mode synonyms and decide if we need to force mode in config
        MODE_INPUT="${{ inputs.mode }}"
        if [ "$MODE_INPUT" = "report-only" ]; then MODE_INPUT="warn"; fi
        if [ "$MODE_INPUT" = "gate" ]; then MODE_INPUT="fail"; fi

        if [ -n "$MODE_INPUT" ]; then
          NEEDS_REWRITE="1"
        fi

        if [ -n "${{ inputs['max-run-cost'] }}" ]; then
          NEEDS_REWRITE="1"
        fi

        if [ "$NEEDS_REWRITE" = "1" ]; then
          echo "Rewriting policy config for requested overrides"
          # Start from original
          cp "${{ inputs.config }}" "$TMP_CONFIG"
          # Ensure budgets block exists if we need to write cost gate
          if [ -n "${{ inputs['max-run-cost'] }}" ]; then
            if ! grep -Eq '^[[:space:]]*budgets:[[:space:]]*$' "$TMP_CONFIG"; then
              printf '\n' >> "$TMP_CONFIG"
              echo 'budgets:' >> "$TMP_CONFIG"
            fi
            # If the key exists, replace it; otherwise, insert under budgets:
            if grep -Eq '^[[:space:]]*cost_usd_total_max:[[:space:]]*' "$TMP_CONFIG"; then
              sed -i.bak -E "s/^[[:space:]]*cost_usd_total_max:[[:space:]]*.*/  cost_usd_total_max: ${{ inputs['max-run-cost'] }}/" "$TMP_CONFIG"
            else
              awk -v val="${{ inputs['max-run-cost'] }}" 'BEGIN{added=0} {print $0; if ($0 ~ /^budgets:\s*$/ && added==0){print "  cost_usd_total_max: " val; added=1}}' "$TMP_CONFIG" > "$TMP_CONFIG.tmp" && mv "$TMP_CONFIG.tmp" "$TMP_CONFIG"
            fi
          fi
          # Force mode if requested (delete all existing then append single key to avoid duplicates)
          if [ -n "$MODE_INPUT" ]; then
            # Remove any existing mode lines (safe against duplicates)
            sed -i.bak -E '/^[[:space:]]*mode:[[:space:]]*/d' "$TMP_CONFIG"
            printf '\nmode: %s\n' "$MODE_INPUT" >> "$TMP_CONFIG"
          fi
          EFFECTIVE_CONFIG="$TMP_CONFIG"
        fi

        # Select CLI package (pin to version with new checks)
        CLI_PACKAGE="${CLI_PACKAGE:-promptproof-cli@0.1.0-beta.3}"

        # Prepare command (force JSON alongside chosen format)
        CMD="npx -y -p $CLI_PACKAGE promptproof eval --config $EFFECTIVE_CONFIG --format ${{ inputs.format }} --out $ARTIFACT_NAME"
        
        # Add mode flag if specified to warn/report-only
        if [ "$MODE_INPUT" = "warn" ]; then CMD="$CMD --warn"; fi
        # Add regress flag if specified
        if [ "${{ inputs.regress }}" = "true" ]; then
          CMD="$CMD --regress"
        fi
        # If baseline-ref provided, imply regress
        if [ -n "${{ inputs['baseline-ref'] }}" ]; then
          CMD="$CMD --regress"
        fi
        # Add seed if specified
        if [ -n "${{ inputs.seed }}" ]; then
          CMD="$CMD --seed ${{ inputs.seed }}"
        fi
        # Add runs if specified
        if [ -n "${{ inputs.runs }}" ]; then
          CMD="$CMD --runs ${{ inputs.runs }}"
        fi
        
        # Run evaluation and capture output
        set +e
        OUTPUT=$($CMD 2>&1)
        EXIT_CODE=$?
        set -e
        
        echo "$OUTPUT"
        
        # Parse output for metrics (prefer JSON report if present)
        JSON_REPORT="$ARTIFACT_NAME.json"
        if [ -f "$JSON_REPORT" ]; then
          VIOLATIONS=$(node -e "console.log(require('./$JSON_REPORT').violations?.length ?? 0)")
          PASSED=$(node -e "console.log(require('./$JSON_REPORT').passed ?? 0)")
          FAILED=$(node -e "console.log(require('./$JSON_REPORT').failed ?? 0)")
          TOTAL_COST=$(node -e "console.log((require('./$JSON_REPORT').budgets?.cost_usd_total ?? 0).toFixed(4))")
          REGRESSIONS=$(node -e "const r=require('./$JSON_REPORT').regression; console.log(r && r.new_failures ? r.new_failures.length : 0)")

          # If HTML is missing (older CLI), synthesize a simple HTML report from JSON
          if [ ! -f "$ARTIFACT_NAME.html" ] && [ "${{ inputs.format }}" = "html" ]; then
            node -e "const fs=require('fs');const r=JSON.parse(fs.readFileSync('$JSON_REPORT','utf8'));function esc(s){return String(s).replace(/&/g,'&amp;').replace(/</g,'&lt;').replace(/>/g,'&gt;')}const rows=(r.violations||[]).slice(0,200).map(v=>`<tr><td>${esc(v.checkId)}</td><td>${esc(v.recordId)}</td><td>${esc(v.message||'')}</td></tr>`).join('');const html=`<!doctype html><meta charset='utf-8'><title>PromptProof Report</title><h1>PromptProof Report</h1><p><b>Total:</b> ${r.total||0} · <b>Passed:</b> ${r.passed||0} · <b>Failed:</b> ${r.failed||0} · <b>Total Cost:</b> $${(r.budgets&&r.budgets.cost_usd_total||0).toFixed(4)}</p><h2>Violations (${(r.violations||[]).length})</h2><table border='1' cellspacing='0' cellpadding='6'><tr><th>Check</th><th>Record</th><th>Message</th></tr>${rows}</table>`;fs.writeFileSync('$ARTIFACT_NAME.html',html);"
          fi
        else
          # If JSON wasn't produced (older CLI), run a second pass to emit JSON deterministically
          echo "JSON report missing after first run; invoking JSON reporter"
          set +e
          OUTPUT_JSON=$(npx -y -p "$CLI_PACKAGE" promptproof eval --config "$EFFECTIVE_CONFIG" --format json --out "$ARTIFACT_NAME" 2>&1)
          EXIT_CODE_JSON=$?
          set -e
          echo "$OUTPUT_JSON"
          
          if [ -f "$JSON_REPORT" ]; then
            VIOLATIONS=$(node -e "console.log(require('./$JSON_REPORT').violations?.length ?? 0)")
            PASSED=$(node -e "console.log(require('./$JSON_REPORT').passed ?? 0)")
            FAILED=$(node -e "console.log(require('./$JSON_REPORT').failed ?? 0)")
            TOTAL_COST=$(node -e "console.log((require('./$JSON_REPORT').budgets?.cost_usd_total ?? 0).toFixed(4))")
            REGRESSIONS=$(node -e "const r=require('./$JSON_REPORT').regression; console.log(r && r.new_failures ? r.new_failures.length : 0)")
            # Synthesize HTML from JSON if requested format is html and file missing
            if [ ! -f "$ARTIFACT_NAME.html" ] && [ "${{ inputs.format }}" = "html" ]; then
              node -e "const fs=require('fs');const r=JSON.parse(fs.readFileSync('$JSON_REPORT','utf8'));function esc(s){return String(s).replace(/&/g,'&amp;').replace(/</g,'&lt;').replace(/>/g,'&gt;')}const rows=(r.violations||[]).slice(0,200).map(v=>`<tr><td>${esc(v.checkId)}</td><td>${esc(v.recordId)}</td><td>${esc(v.message||'')}</td></tr>`).join('');const html=`<!doctype html><meta charset='utf-8'><title>PromptProof Report</title><h1>PromptProof Report</h1><p><b>Total:</b> ${r.total||0} · <b>Passed:</b> ${r.passed||0} · <b>Failed:</b> ${r.failed||0} · <b>Total Cost:</b> $${(r.budgets&&r.budgets.cost_usd_total||0).toFixed(4)}</p><h2>Violations (${(r.violations||[]).length})</h2><table border='1' cellspacing='0' cellpadding='6'><tr><th>Check</th><th>Record</th><th>Message</th></tr>${rows}</table>`;fs.writeFileSync('$ARTIFACT_NAME.html',html);"
            fi
          else
          VIOLATIONS=$(echo "$OUTPUT" | grep -oP '\\d+ violations found' | grep -oP '\\d+' || echo "0")
          PASSED=$(echo "$OUTPUT" | grep -oP 'Passed: \\d+' | grep -oP '\\d+' || echo "0")
          FAILED=$(echo "$OUTPUT" | grep -oP 'Failed: \\d+' | grep -oP '\\d+' || echo "0")
          TOTAL_COST="0.0000"
          REGRESSIONS="0"

          # Fallback: emit simple HTML and JSON if reporter files are missing
          echo "No JSON report found; creating fallback report files"
          {
            echo "<html><head><meta charset=\"utf-8\"><title>PromptProof Report</title></head><body>"
            echo "<h1>PromptProof Report (fallback)</h1>"
            echo "<p><strong>Note:</strong> Using console output because JSON/HTML report was not generated by the CLI.</p>"
            echo "<pre>"
            echo "$OUTPUT" | sed 's/&/\&amp;/g; s/</\&lt;/g; s/>/\&gt;/g'
            echo "</pre>"
            echo "</body></html>"
          } > "$ARTIFACT_NAME.html"

          # Write minimal JSON next to HTML
          echo '{"total":0,"passed":0,"failed":0,"violations":[],"budgets":{"cost_usd_total":0,"latency_ms_p95":0,"latency_ms_p99":0},"mode":"warn","exitCode":0}' > "$ARTIFACT_NAME.json"
          fi
        fi
        
        # Set outputs
        echo "violations=$VIOLATIONS" >> $GITHUB_OUTPUT
        echo "passed=$PASSED" >> $GITHUB_OUTPUT
        echo "failed=$FAILED" >> $GITHUB_OUTPUT
        # Map format to file extension
        EXT="txt"
        case "${{ inputs.format }}" in
          html) EXT="html";;
          junit) EXT="xml";;
          json) EXT="json";;
          sarif) EXT="sarif";;
          console) EXT="txt";;
        esac
        echo "report-path=$ARTIFACT_NAME.$EXT" >> $GITHUB_OUTPUT
        echo "exit-code=$EXIT_CODE" >> $GITHUB_OUTPUT
        echo "total-cost=$TOTAL_COST" >> $GITHUB_OUTPUT
        echo "failed-tests=$FAILED" >> $GITHUB_OUTPUT
        echo "regressions=$REGRESSIONS" >> $GITHUB_OUTPUT
        
        # Create summary
        echo "## PromptProof Evaluation Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "$VIOLATIONS" = "0" ]; then
          echo "✅ **All checks passed!**" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **$VIOLATIONS violations found**" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- Fixtures evaluated: $((PASSED + FAILED))" >> $GITHUB_STEP_SUMMARY
        echo "- Passed: $PASSED" >> $GITHUB_STEP_SUMMARY
        echo "- Failed: $FAILED" >> $GITHUB_STEP_SUMMARY
        echo "- Total Cost: \$$TOTAL_COST" >> $GITHUB_STEP_SUMMARY
        if [ "$REGRESSIONS" != "0" ]; then
          echo "- Regressions: $REGRESSIONS" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "::endgroup::"
        
        # Exit with original code
        exit $EXIT_CODE

    - name: Upload Report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: ${{ inputs['report-artifact'] }}-${{ github.job }}-${{ github.run_attempt }}
        path: |
          ${{ inputs['report-artifact'] }}.*
        retention-days: 30
        overwrite: true

    - name: Upload SARIF to Code Scanning
      if: always() && inputs.format == 'sarif'
      uses: github/codeql-action/upload-sarif@v3
      with:
        sarif_file: ${{ inputs['report-artifact'] }}.sarif
        category: promptproof

    - name: Comment on PR
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      env:
        REPORT_BASENAME: ${{ inputs['report-artifact'] }}
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read the JSON report if it exists
          let reportData = null;
          const base = process.env.REPORT_BASENAME || 'promptproof-report';
          const jsonPath = `${base}.json`;
          
          if (fs.existsSync(jsonPath)) {
            reportData = JSON.parse(fs.readFileSync(jsonPath, 'utf8'));
          }
          
          // Build comment with artifact and run links
          let comment = '## 🔍 PromptProof Evaluation\n\n';
          
          if (reportData) {
            const { violations, total, passed, failed, mode, budgets } = reportData;
            
            if (violations.length === 0) {
              comment += '✅ **All checks passed!**\n\n';
            } else {
              comment += `❌ **${violations.length} violation(s) found**\n\n`;
              
              // Group violations by check
              const byCheck = {};
              violations.forEach(v => {
                if (!byCheck[v.checkId]) byCheck[v.checkId] = [];
                byCheck[v.checkId].push(v);
              });
              
              comment += '### Violations\n\n';
              for (const [checkId, checkViolations] of Object.entries(byCheck)) {
                comment += `<details>\n<summary><code>${checkId}</code> (${checkViolations.length} violations)</summary>\n\n`;
                checkViolations.slice(0, 5).forEach(v => {
                  comment += `- Record \`${v.recordId}\`: ${v.message}\n`;
                });
                if (checkViolations.length > 5) {
                  comment += `- _...and ${checkViolations.length - 5} more_\n`;
                }
                comment += '\n</details>\n\n';
              }
            }
            
            // Add metrics
            comment += '### Metrics\n\n';
            comment += `| Metric | Value |\n`;
            comment += `|--------|-------|\n`;
            comment += `| Total Fixtures | ${total} |\n`;
            comment += `| Passed | ${passed} |\n`;
            comment += `| Failed | ${failed} |\n`;
            comment += `| Total Cost | $${budgets.cost_usd_total?.toFixed(4) || '0.0000'} |\n`;
            comment += `| P95 Latency | ${budgets.latency_ms_p95}ms |\n`;
            comment += `| Mode | ${mode} |\n`;
            
          } else {
            comment += '⚠️ Could not load evaluation results\n';
          }
          
          // Artifact link
          const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
          const jobName = process.env.GITHUB_JOB || context.job || '';
          const artifactPrefix = process.env.REPORT_BASENAME || 'promptproof-report';
          const expectedName = `${artifactPrefix}-${jobName}-${context.runAttempt}`;
          let artifactUrl = runUrl;
          try {
            const { data } = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.runId,
              per_page: 100
            });
            const match = data.artifacts.find(a => a.name === expectedName) || data.artifacts.find(a => a.name.startsWith(artifactPrefix));
            if (match) {
              artifactUrl = `${runUrl}/artifacts/${match.id}`;
            }
          } catch (e) {
            // ignore; fall back to run URL
          }
          comment += `\n**Artifacts**: [Open in run](${artifactUrl})`;
          comment += ` • Format: \`${'${{ inputs.format }}'}\``;

          comment += '\n---\n';
          comment += '_Generated by [PromptProof](https://github.com/geminimir/promptproof)_';
          
          // Find existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const botComment = comments.find(comment => 
            comment.user.type === 'Bot' && 
            comment.body.includes('PromptProof Evaluation')
          );
          
          if (botComment) {
            // Update existing comment
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: comment
            });
          } else {
            // Create new comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
          }
