name: 'PromptProof Eval'
description: 'Deterministic replay and policy checks for LLM outputs'
author: 'PromptProof'
branding:
  icon: 'check-circle'
  color: 'green'

inputs:
  config:
    description: 'Path to promptproof.yaml configuration file'
    required: false
    default: 'promptproof.yaml'
  format:
    description: 'Output format (console|html|junit|json)'
    required: false
    default: 'html'
  mode:
    description: 'Evaluation mode (gate|report-only|fail|warn). Defaults to policy mode.'
    required: false
    default: ''
  node-version:
    description: 'Node.js version to use'
    required: false
    default: '20'
  regress:
    description: 'Compare against baseline snapshot (true|false)'
    required: false
    default: ''
  baseline-ref:
    description: 'Git ref to load baseline snapshot from (e.g., origin/main). Implies regress.'
    required: false
    default: ''
  seed:
    description: 'Seed for non-deterministic checks'
    required: false
    default: ''
  runs:
    description: 'Number of runs for non-deterministic checks'
    required: false
    default: ''
  max-run-cost:
    description: 'Maximum total cost for this run in USD (overrides policy budgets.cost_usd_total_max)'
    required: false
    default: ''
  snapshot-on-success:
    description: 'Create a snapshot after a successful run (true|false)'
    required: false
    default: ''
  snapshot-promote-on-main:
    description: 'Promote snapshot to baseline when on main branch (true|false)'
    required: false
    default: ''
  snapshot-tag:
    description: 'Optional snapshot tag to use'
    required: false
    default: ''
  report-artifact:
    description: 'Name of the uploaded report artifact (without extension)'
    required: false
    default: 'promptproof-report'

outputs:
  violations:
    description: 'Number of violations found'
    value: ${{ steps.eval.outputs.violations }}
  passed:
    description: 'Number of fixtures that passed'
    value: ${{ steps.eval.outputs.passed }}
  failed:
    description: 'Number of fixtures that failed'
    value: ${{ steps.eval.outputs.failed }}
  report-path:
    description: 'Path to the generated report'
    value: ${{ steps.eval.outputs.report-path }}
  exit-code:
    description: 'Exit code from evaluation'
    value: ${{ steps.eval.outputs.exit-code }}
  total-cost:
    description: 'Total cost (USD) for the evaluated fixtures'
    value: ${{ steps.eval.outputs.total-cost }}
  failed-tests:
    description: 'Alias for failed (number of fixtures that failed)'
    value: ${{ steps.eval.outputs.failed-tests }}
  regressions:
    description: 'Number of new failures found versus baseline (when regression comparison is enabled)'
    value: ${{ steps.eval.outputs.regressions }}

runs:
  using: 'composite'
  steps:
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ inputs.node-version }}

    - name: Install PromptProof CLI
      shell: bash
      run: |
        echo "::group::Installing PromptProof CLI"
        npm install -g promptproof-cli@latest
        echo "::endgroup::"

    - name: Run PromptProof Evaluation
      id: eval
      shell: bash
      run: |
        echo "::group::Running PromptProof evaluation"
        
        # Determine effective config path (may be rewritten below)
        EFFECTIVE_CONFIG="${{ inputs.config }}"

        # Prepare artifact base name and output path
        ARTIFACT_NAME="${{ inputs['report-artifact'] }}"
        if [ -z "$ARTIFACT_NAME" ]; then
          ARTIFACT_NAME="promptproof-report"
        fi

        # Handle baseline-ref: fetch baseline snapshot files from the given ref
        if [ -n "${{ inputs['baseline-ref'] }}" ]; then
          echo "Fetching baseline snapshot from ref: ${{ inputs['baseline-ref'] }}"
          mkdir -p .promptproof/baselines
          # Find all last_green.json files at the ref
          SUITE_FILES=$(git ls-tree -r --name-only "${{ inputs['baseline-ref'] }}" .promptproof/baselines 2>/dev/null | grep 'last_green.json' || true)
          if [ -n "$SUITE_FILES" ]; then
            while read -r FILE; do
              [ -z "$FILE" ] && continue
              SUITE=$(echo "$FILE" | awk -F/ '{print $3}')
              mkdir -p ".promptproof/baselines/$SUITE"
              git show "${{ inputs['baseline-ref'] }}:$FILE" > ".promptproof/baselines/$SUITE/last_green.json" || true
              BASELINE_JSON=".promptproof/baselines/$SUITE/last_green.json"
              if [ -f "$BASELINE_JSON" ]; then
                MANIFEST_REL=$(node -e "try{const b=require('./$BASELINE_JSON');console.log(b.path||'')}catch(e){console.log('')}")
                TAG=$(node -e "try{const b=require('./$BASELINE_JSON');console.log(b.tag||'')}catch(e){console.log('')}")
                if [ -n "$MANIFEST_REL" ]; then
                  mkdir -p ".promptproof/baselines/$SUITE/$MANIFEST_REL"
                  git show "${{ inputs['baseline-ref'] }}:.promptproof/baselines/$SUITE/$MANIFEST_REL/manifest.json" > ".promptproof/baselines/$SUITE/$MANIFEST_REL/manifest.json" 2>/dev/null || true
                fi
                if [ -n "$TAG" ]; then
                  mkdir -p ".promptproof/snapshots/$SUITE/$TAG"
                  git show "${{ inputs['baseline-ref'] }}:.promptproof/snapshots/$SUITE/$TAG/manifest.json" > ".promptproof/snapshots/$SUITE/$TAG/manifest.json" 2>/dev/null || true
                fi
              fi
            done <<< "$SUITE_FILES"
          else
            echo "No baseline snapshots found at ref ${{ inputs['baseline-ref'] }}" || true
          fi
        fi

        # Possibly rewrite config to enforce mode and/or max-run-cost
        NEEDS_REWRITE="0"
        TMP_CONFIG="promptproof.effective.yaml"

        # Normalize mode synonyms and decide if we need to force mode in config
        MODE_INPUT="${{ inputs.mode }}"
        if [ "$MODE_INPUT" = "report-only" ]; then MODE_INPUT="warn"; fi
        if [ "$MODE_INPUT" = "gate" ]; then MODE_INPUT="fail"; fi

        if [ -n "$MODE_INPUT" ]; then
          NEEDS_REWRITE="1"
        fi

        if [ -n "${{ inputs['max-run-cost'] }}" ]; then
          NEEDS_REWRITE="1"
        fi

        if [ "$NEEDS_REWRITE" = "1" ]; then
          echo "Rewriting policy config for requested overrides"
          # Start from original
          cp "${{ inputs.config }}" "$TMP_CONFIG"
          # Ensure budgets block exists if we need to write cost gate
          if [ -n "${{ inputs['max-run-cost'] }}" ]; then
            if ! grep -Eq '^\s*budgets:\s*$' "$TMP_CONFIG"; then
              printf '\n' >> "$TMP_CONFIG"
              echo 'budgets:' >> "$TMP_CONFIG"
            fi
            # If the key exists, replace it; otherwise, insert under budgets:
            if grep -Eq '^\s*cost_usd_total_max:\s*' "$TMP_CONFIG"; then
              sed -i.bak -E "s/^\s*cost_usd_total_max:\s*.*/  cost_usd_total_max: ${{ inputs['max-run-cost'] }}/" "$TMP_CONFIG"
            else
              awk -v val="${{ inputs['max-run-cost'] }}" 'BEGIN{added=0} {print $0; if ($0 ~ /^budgets:\s*$/ && added==0){print "  cost_usd_total_max: " val; added=1}}' "$TMP_CONFIG" > "$TMP_CONFIG.tmp" && mv "$TMP_CONFIG.tmp" "$TMP_CONFIG"
            fi
          fi
          # Force mode if requested (replace existing or append)
          if [ -n "$MODE_INPUT" ]; then
            if grep -Eq '^\s*mode:\s*' "$TMP_CONFIG"; then
              sed -i.bak -E "s/^\s*mode:\s*.*/mode: $MODE_INPUT/" "$TMP_CONFIG"
            else
              printf '\nmode: %s\n' "$MODE_INPUT" >> "$TMP_CONFIG"
            fi
          fi
          EFFECTIVE_CONFIG="$TMP_CONFIG"
        fi

        # Prepare command
        CMD="promptproof eval --config $EFFECTIVE_CONFIG --format ${{ inputs.format }} --out $ARTIFACT_NAME"
        
        # Add mode flag if specified to warn/report-only
        if [ "$MODE_INPUT" = "warn" ]; then CMD="$CMD --warn"; fi
        # Add regress flag if specified
        if [ "${{ inputs.regress }}" = "true" ]; then
          CMD="$CMD --regress"
        fi
        # If baseline-ref provided, imply regress
        if [ -n "${{ inputs['baseline-ref'] }}" ]; then
          CMD="$CMD --regress"
        fi
        # Add seed if specified
        if [ -n "${{ inputs.seed }}" ]; then
          CMD="$CMD --seed ${{ inputs.seed }}"
        fi
        # Add runs if specified
        if [ -n "${{ inputs.runs }}" ]; then
          CMD="$CMD --runs ${{ inputs.runs }}"
        fi
        
        # Run evaluation and capture output
        set +e
        OUTPUT=$($CMD 2>&1)
        EXIT_CODE=$?
        set -e
        
        echo "$OUTPUT"
        
        # Parse output for metrics (prefer JSON report if present)
        JSON_REPORT="$ARTIFACT_NAME.json"
        if [ -f "$JSON_REPORT" ]; then
          VIOLATIONS=$(node -e "console.log(require('./$JSON_REPORT').violations?.length ?? 0)")
          PASSED=$(node -e "console.log(require('./$JSON_REPORT').passed ?? 0)")
          FAILED=$(node -e "console.log(require('./$JSON_REPORT').failed ?? 0)")
          TOTAL_COST=$(node -e "console.log((require('./$JSON_REPORT').budgets?.cost_usd_total ?? 0).toFixed(4))")
          REGRESSIONS=$(node -e "const r=require('./$JSON_REPORT').regression; console.log(r && r.new_failures ? r.new_failures.length : 0)")
        else
          VIOLATIONS=$(echo "$OUTPUT" | grep -oP '\\d+ violations found' | grep -oP '\\d+' || echo "0")
          PASSED=$(echo "$OUTPUT" | grep -oP 'Passed: \\d+' | grep -oP '\\d+' || echo "0")
          FAILED=$(echo "$OUTPUT" | grep -oP 'Failed: \\d+' | grep -oP '\\d+' || echo "0")
          TOTAL_COST="0.0000"
          REGRESSIONS="0"

          # Fallback: emit simple HTML and JSON if reporter files are missing
          echo "No JSON report found; creating fallback report files"
          {
            echo "<html><head><meta charset=\"utf-8\"><title>PromptProof Report</title></head><body>"
            echo "<h1>PromptProof Report (fallback)</h1>"
            echo "<p><strong>Note:</strong> Using console output because JSON/HTML report was not generated by the CLI.</p>"
            echo "<pre>"
            echo "$OUTPUT" | sed 's/&/\&amp;/g; s/</\&lt;/g; s/>/\&gt;/g'
            echo "</pre>"
            echo "</body></html>"
          } > "$ARTIFACT_NAME.html"

          cat > "$ARTIFACT_NAME.json" <<'EOF'
{"total":0,"passed":0,"failed":0,"violations":[],"budgets":{"cost_usd_total":0,"latency_ms_p95":0,"latency_ms_p99":0},"mode":"warn","exitCode":0}
EOF
        fi
        
        # Set outputs
        echo "violations=$VIOLATIONS" >> $GITHUB_OUTPUT
        echo "passed=$PASSED" >> $GITHUB_OUTPUT
        echo "failed=$FAILED" >> $GITHUB_OUTPUT
        # Map format to file extension
        EXT="txt"
        case "${{ inputs.format }}" in
          html) EXT="html";;
          junit) EXT="xml";;
          json) EXT="json";;
          console) EXT="txt";;
        esac
        echo "report-path=$ARTIFACT_NAME.$EXT" >> $GITHUB_OUTPUT
        echo "exit-code=$EXIT_CODE" >> $GITHUB_OUTPUT
        echo "total-cost=$TOTAL_COST" >> $GITHUB_OUTPUT
        echo "failed-tests=$FAILED" >> $GITHUB_OUTPUT
        echo "regressions=$REGRESSIONS" >> $GITHUB_OUTPUT
        
        # Create summary
        echo "## PromptProof Evaluation Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "$VIOLATIONS" = "0" ]; then
          echo "âœ… **All checks passed!**" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **$VIOLATIONS violations found**" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- Fixtures evaluated: $((PASSED + FAILED))" >> $GITHUB_STEP_SUMMARY
        echo "- Passed: $PASSED" >> $GITHUB_STEP_SUMMARY
        echo "- Failed: $FAILED" >> $GITHUB_STEP_SUMMARY
        echo "- Total Cost: \$$TOTAL_COST" >> $GITHUB_STEP_SUMMARY
        if [ "$REGRESSIONS" != "0" ]; then
          echo "- Regressions: $REGRESSIONS" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "::endgroup::"
        
        # Exit with original code
        exit $EXIT_CODE

    - name: Upload Report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: ${{ inputs['report-artifact'] }}
        path: |
          ${{ inputs['report-artifact'] }}.*
        retention-days: 30

    - name: Comment on PR
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      env:
        REPORT_BASENAME: ${{ inputs['report-artifact'] }}
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read the JSON report if it exists
          let reportData = null;
          const base = process.env.REPORT_BASENAME || 'promptproof-report';
          const jsonPath = `${base}.json`;
          
          if (fs.existsSync(jsonPath)) {
            reportData = JSON.parse(fs.readFileSync(jsonPath, 'utf8'));
          }
          
          // Build comment
          let comment = '## ðŸ” PromptProof Evaluation\n\n';
          
          if (reportData) {
            const { violations, total, passed, failed, mode, budgets } = reportData;
            
            if (violations.length === 0) {
              comment += 'âœ… **All checks passed!**\n\n';
            } else {
              comment += `âŒ **${violations.length} violation(s) found**\n\n`;
              
              // Group violations by check
              const byCheck = {};
              violations.forEach(v => {
                if (!byCheck[v.checkId]) byCheck[v.checkId] = [];
                byCheck[v.checkId].push(v);
              });
              
              comment += '### Violations\n\n';
              for (const [checkId, checkViolations] of Object.entries(byCheck)) {
                comment += `<details>\n<summary><code>${checkId}</code> (${checkViolations.length} violations)</summary>\n\n`;
                checkViolations.slice(0, 5).forEach(v => {
                  comment += `- Record \`${v.recordId}\`: ${v.message}\n`;
                });
                if (checkViolations.length > 5) {
                  comment += `- _...and ${checkViolations.length - 5} more_\n`;
                }
                comment += '\n</details>\n\n';
              }
            }
            
            // Add metrics
            comment += '### Metrics\n\n';
            comment += `| Metric | Value |\n`;
            comment += `|--------|-------|\n`;
            comment += `| Total Fixtures | ${total} |\n`;
            comment += `| Passed | ${passed} |\n`;
            comment += `| Failed | ${failed} |\n`;
            comment += `| Total Cost | $${budgets.cost_usd_total?.toFixed(4) || '0.0000'} |\n`;
            comment += `| P95 Latency | ${budgets.latency_ms_p95}ms |\n`;
            comment += `| Mode | ${mode} |\n`;
            
          } else {
            comment += 'âš ï¸ Could not load evaluation results\n';
          }
          
          comment += '\n---\n';
          comment += '_Generated by [PromptProof](https://github.com/geminimir/promptproof)_';
          
          // Find existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const botComment = comments.find(comment => 
            comment.user.type === 'Bot' && 
            comment.body.includes('PromptProof Evaluation')
          );
          
          if (botComment) {
            // Update existing comment
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: comment
            });
          } else {
            // Create new comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
          }
